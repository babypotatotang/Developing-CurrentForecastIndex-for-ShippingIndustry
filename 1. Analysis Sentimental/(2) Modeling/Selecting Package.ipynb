{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c27aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "##어떤 클래스를 사용할지 \n",
    "## 각 클래스를 사용해서 5개 기사에 대해서 비교 후 선정하는 걸로\n",
    "## 클래스는 okt, komoran, mecab, kkma,Hannanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a959f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "import pandas as pd\n",
    "import re  \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d64ca077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(text):  \n",
    "    new_text=' '.join(text.split()) #공백 제거\n",
    "    pattern = '[0-9]'\n",
    "    new_text = re.sub(pattern=pattern,repl=' ',string=new_text)\n",
    "    \n",
    "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' #이메일 주소 제거\n",
    "    new_text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    \n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:\\da-fA-F]{2}))+'  # url 제거\n",
    "    new_text = re.sub(pattern=pattern,repl=' ',string=new_text)\n",
    "\n",
    "    pattern = '<[^>]*>'                                          # html tag 제거\n",
    "    new_text = re.sub(pattern=pattern,repl=' ',string=new_text)\n",
    "\n",
    "    pattern = '[\\r|\\n]'                                          # \\r, \\n 제거\n",
    "    new_text = re.sub(pattern=pattern,repl=' ',string=new_text)\n",
    "\n",
    "    pattern = '[^\\w\\s]'                                          # 특수기호 제거\n",
    "    new_text = re.sub(pattern=pattern,repl=' ',string=new_text)\n",
    "\n",
    "    pattern = re.compile(r'\\s+')                                 # 이중 space 제거\n",
    "    new_text = re.sub(pattern=pattern,repl=' ',string=new_text)\n",
    "    \n",
    "    \n",
    "    ##불용어 처리\n",
    "    stopword=\"\\d|급$|씨$|월$|일$|더불어민주당|새누리당|야당|여당|사진|이날|_|기자$|위|등\" \n",
    "\n",
    "    word_tokens=word_tokenize(new_text)\n",
    "    data_word=[x for x in word_tokens if not re.search(stopword,x)]\n",
    "    new_text=\" \".join(data_word)\n",
    "    \n",
    "\n",
    "    return(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67915af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus(tmp_list):\n",
    "    return ' '+' '.join(tmp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1e4ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson(tf_idf_matrix):\n",
    "    array=tf_idf_matrix.toarray()\n",
    "    \n",
    "    m=len(array[0])\n",
    "    TFa=sum(array[0])\n",
    "    TFb=sum(array[1])\n",
    "\n",
    "    sum_w=dot(array[0],array[1])\n",
    "\n",
    "    sum_wa2=sum(array[0]**2); sum_wb2=sum(array[1]**2)\n",
    "    norma=m*sum_wa2-TFa**2; normb=m*sum_wb2-TFb**2\n",
    "\n",
    "    up=(m*sum_w)-(TFa*TFb)\n",
    "    down=math.sqrt(float(norma*normb))\n",
    "\n",
    "    return up/down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4020d1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.591504</td>\n",
       "      <td>okt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.621768</td>\n",
       "      <td>komo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.327784</td>\n",
       "      <td>kkma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.593158</td>\n",
       "      <td>han</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Value   Version\n",
       "0  1.000000  original\n",
       "1  0.591504       okt\n",
       "2  0.621768      komo\n",
       "3  0.327784      kkma\n",
       "4  0.593158       han"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Random_Docs=pd.read_excel('D:\\\\KMI-Project_tmp\\\\Data\\\\Sentimental\\\\RandomDocs15.xlsx') # 랜덤 기사 문서\n",
    "origin_list=[]\n",
    "okt=Okt(); okt_doc=''\n",
    "komo=Komoran(); komo_doc=''\n",
    "kkma=Kkma(); kkma_doc=''\n",
    "han=Hannanum(); han_doc=''\n",
    "package_list=['original','okt','komo','kkma','han'] #사용된 한국어 패키지\n",
    "\n",
    "pearson_df=pd.DataFrame() \n",
    "total_docs=[];\n",
    "\n",
    "for index,row in Random_Docs.iterrows(): #랜덤 기사의 순서대로 iteration 진행 \n",
    "    \n",
    "    doc=cleansing(row['본문']); origin_list.append(row['키워드'])\n",
    "    #original 본문에 대해서 각 패키지를 활용하여 토큰화 진행 \n",
    "    okt_doc+=corpus(okt.nouns(doc)); komo_doc+=corpus(komo.nouns(doc))\n",
    "    kkma_doc+=corpus(kkma.nouns(doc)); han_doc+=corpus(han.nouns(doc))\n",
    "\n",
    "origin_doc=corpus(origin_list)\n",
    "total_docs.append(origin_doc); total_docs.append(okt_doc); total_docs.append(komo_doc)\n",
    "total_docs.append(kkma_doc); total_docs.append(han_doc)\n",
    "    \n",
    "for index in range(5):\n",
    "    comp_doclist=[]\n",
    "    comp_doclist.append(total_docs[0]); comp_doclist.append(total_docs[index])\n",
    "        #비교할 문서리스트 선언: total_docs[0]: 빅카인즈의 키워드, total_docs[index]: 각각 순서대로 패키지의 산출물 \n",
    "    tfidf_matrix=TfidfVectorizer().fit_transform(comp_doclist) \n",
    "        #tfidf 행렬을 이용(논문 참고)\n",
    "    pearson_df=pearson_df.append(pd.Series([package_list[index],pearson(tfidf_matrix)],index=['Version','Value']),ignore_index=True)\n",
    "\n",
    "display(pearson_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1494c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## komo 패키지 사용~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykernel",
   "language": "python",
   "name": "pykernel3.7.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
