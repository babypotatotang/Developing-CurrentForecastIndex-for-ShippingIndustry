{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(3) Sentiment Classifier Modeling.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babypotatotang/Developing-CurrentForecastIndex-for-ShippingIndustry/blob/main/1.%20Analysis%20Sentimental/(2)%20Modeling/(3)%20Modeling%20Sentiment%20Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHQxz8L6OhvG"
      },
      "outputs": [],
      "source": [
        "# Colab에 Mecab 설치\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import Counter\n",
        "from konlpy.tag import Mecab\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from google.colab import files "
      ],
      "metadata": {
        "id": "tph6WSLLOkcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 load 및 기본 스펙 확인 : train_data, test_data"
      ],
      "metadata": {
        "id": "eZFw9VuCFNAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=pd.read_csv(filename),usecols=['Sentence','Label'],encoding='cp949')\n",
        "test_data=pd.read_csv(filename,usecols=['Sentence','Label'],encoding='cp949')\n",
        "\\"
      ],
      "metadata": {
        "id": "bGa_FX-n6_8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['Sentence'].nunique(), train_data['Label'].nunique() "
      ],
      "metadata": {
        "id": "KlS4ZTOV9EGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['Sentence'].nunique(), test_data['Label'].nunique()"
      ],
      "metadata": {
        "id": "Ptzn7QNI9Pj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['Label']=train_data['Label'].astype(int)\n",
        "train_data['Label']=train_data['Label'].apply(lambda x: 0 if x==-1 else x) #부정 레이블 0으로 설정\n",
        "\n",
        "test_data['Label']=test_data['Label'].astype(int)\n",
        "test_data['Label']=test_data['Label'].apply(lambda x: 0 if x==-1 else x) #부정 레이블 0으로 설정"
      ],
      "metadata": {
        "id": "Sw6eJcxotVZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.drop_duplicates(subset = ['Sentence'], inplace=True) # 중복 제거\n",
        "train_data['Sentence'] = train_data['Sentence'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") #한글과 공백을 제외하고 모두 제거\n",
        "train_data['Sentence'].replace('', np.nan, inplace=True)#값이 없는 경우 Null 값으로 변경\n",
        "train_data.dropna(how='any',inplace=True) # Null 값 제거\n",
        "\n",
        "print('전처리 후 훈련용 샘플의 개수 :',len(train_data))\n",
        "print('null값 존재 유무: ',train_data.isnull().values.any())"
      ],
      "metadata": {
        "id": "vSI3sA4j7Zc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.drop_duplicates(subset = ['Sentence'], inplace=True) # 중복 제거\n",
        "test_data['Sentence'] = test_data['Sentence'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") #한글과 공백을 제외하고 모두 제거\n",
        "test_data['Sentence'].replace('', np.nan, inplace=True)#값이 없는 경우 Null 값으로 변경\n",
        "test_data.dropna(how='any',inplace=True) # Null 값 제거\n",
        "\n",
        "print('훈련용 데이터 개수: ',len(test_data))\n",
        "print('null값 존재 유무: ',test_data.isnull().values.any())"
      ],
      "metadata": {
        "id": "EHqaeUsV7-AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 전처리: sentence 토큰화"
      ],
      "metadata": {
        "id": "-NP4BMPx9WSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #tokenized를 list로 변경\n",
        "mecab=Mecab()\n",
        "stopwords = ['했','있','으로','로','것','씨','말','도', '는', '다', '의', '가', '이', '은','수','에서'\n",
        "             '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게', '만', '겜', '되', '음', '면']"
      ],
      "metadata": {
        "id": "15F8c7IXGINn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Train Data] 전처리\n",
        "train_data['tokenized']=train_data['Sentence'].apply(mecab.morphs) #Sentence 내용을 morphs로 형태소 분석(type: list)\n",
        "train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords]) #해당 열의 값 중 stopword에 해당하는 값 지우기\n",
        "train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if len(item)>1]) #길이 2이상만 저장"
      ],
      "metadata": {
        "id": "E8Y3d4gvFSoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Test Data] 전처리\n",
        "test_data['tokenized']=test_data['Sentence'].apply(mecab.morphs) #Sentence 내용을 morphs로 형태소 분석(type: list)\n",
        "test_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords]) #해당 열의 값 중 stopword에 해당하는 값 지우기\n",
        "test_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if len(item)>1]) #길이 2이상만 저장"
      ],
      "metadata": {
        "id": "0v8_sb5ZF_Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#데이터 전처리: common word 제거"
      ],
      "metadata": {
        "id": "11-csZOAcwpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def len_by_sentimental(df):\n",
        "    positive_words = np.hstack(df[df.Label == 1]['tokenized'].values)\n",
        "    positive_word_count = Counter(positive_words)\n",
        "\n",
        "    negative_words = np.hstack(df[df.Label == 0]['tokenized'].values)\n",
        "    negative_word_count = Counter(negative_words)\n",
        "\n",
        "    print('긍정 단어 길이: ',len(positive_word_count))\n",
        "    print('부정 단어 길이: ',len(negative_word_count))\n",
        "\n",
        "    return negative_word_count,positive_word_count"
      ],
      "metadata": {
        "id": "ORqahGUfXdvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def common_word_list(common_num,neg,pos):\n",
        "    negative_word=[]; positive_word=[]\n",
        "    n_list=neg.most_common(common_num); p_list=pos.most_common(common_num)\n",
        "\n",
        "    for i in range(common_num):\n",
        "        negative_word.append(n_list[i][0])\n",
        "        positive_word.append(p_list[i][0])\n",
        "\n",
        "    common_list=list(set(negative_word) & set(positive_word))\n",
        "\n",
        "    print(common_list)\n",
        "    print('common_list 길이', len(common_list))\n",
        "\n",
        "    return common_list"
      ],
      "metadata": {
        "id": "gDtxsL1LaPUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[Train Data]\n",
        "train_neg,train_pos=len_by_sentimental(train_data) #common_word 산출 범위(num range) 결정\n",
        "common_list=common_word_list(20,train_neg,train_pos)\n",
        "train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in common_list]) #해당 열의 값 중 stopword에 해당하는 값 지우기"
      ],
      "metadata": {
        "id": "Pf69B8bVQJTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[Test Data]\n",
        "test_neg,test_pos=len_by_sentimental(test_data) #common_word 산출 범위(num range) 결정\n",
        "common_list=common_word_list(20,test_neg,test_pos)\n",
        "test_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in common_list]) #해당 열의 값 중 stopword에 해당하는 값 지우기"
      ],
      "metadata": {
        "id": "myRfbUEEeI46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.to_csv(filename)\n",
        "test_data.to_csv(filename)"
      ],
      "metadata": {
        "id": "mYZRYOVLu5MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_words = np.hstack(train_data[train_data.Label == 0]['tokenized'].values)\n",
        "negative_word_count = Counter(negative_words)\n",
        "print(negative_word_count.most_common(20))"
      ],
      "metadata": {
        "id": "7mcVJY69PQyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리 후 그래프 확인"
      ],
      "metadata": {
        "id": "qR1Zm0YFdDAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('전처리 후 훈련용 샘플의 개수 :',len(train_data))\n",
        "print('null값 존재 유무: ',train_data.isnull().values.any())"
      ],
      "metadata": {
        "id": "GN_BqQZQSXly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('전처리 후 훈련용 샘플의 개수 :',len(test_data))\n",
        "print('null값 존재 유무: ',test_data.isnull().values.any())"
      ],
      "metadata": {
        "id": "kOL-iz4iSY33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_data['tokenized'].values\n",
        "y_train = train_data['Label'].values\n",
        "X_test= test_data['tokenized'].values\n",
        "y_test = test_data['Label'].values"
      ],
      "metadata": {
        "id": "ZFFNSJCnF_e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Sentence) 정수 인코딩"
      ],
      "metadata": {
        "id": "U4w60n8gGtFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train) #문자데이터를 입력받아 리스트 형태로 변환, 각 단어에 index 부여"
      ],
      "metadata": {
        "id": "zzgUBbibF_id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 6 #단어의 등장 빈도수에 대한 임계치 \n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
        "\n",
        "#사용되는 단어 집합의 크기 \n",
        "vocab_size = total_cnt - rare_cnt + 2 #0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
        "print('단어 집합의 크기 :',vocab_size)"
      ],
      "metadata": {
        "id": "41E8m3KgF_lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') #새 vocab_size로 tokenizer 새로 설정\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "#X_train, X_test의 데이터를 넣어서 인코딩 \n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "j7hf2qy4Ho9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Sentence) 패딩"
      ],
      "metadata": {
        "id": "W-d1R8rdHsyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('뉴스의 최대 길이 :',max(len(l) for l in X_train)) \n",
        "print('뉴스의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
        "plt.hist([len(s) for s in X_train], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j5EG061aHykU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def below_threshold_len(max_len, nested_list):\n",
        "# 희귀 단어의 개수만큼 제거하는 함수, max_len은 리뷰의 최대 및 평균 길이를 보고 비교해서 설정\n",
        "  count = 0\n",
        "  for sentence in nested_list:\n",
        "    if(len(sentence) <= max_len):\n",
        "        count = count + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))"
      ],
      "metadata": {
        "id": "aFDIjaaHH5Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 1000\n",
        "below_threshold_len(max_len, X_train)"
      ],
      "metadata": {
        "id": "YDKG6wl5H7AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pad_sequences(X_train, maxlen = max_len)\n",
        "X_test = pad_sequences(X_test, maxlen = max_len)"
      ],
      "metadata": {
        "id": "_jM6V1t1H_bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 머신러닝 모델링"
      ],
      "metadata": {
        "id": "Xlj3MJE2IFgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "dZx6W3QMIKy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(Bidirectional(LSTM(hidden_units)))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "uKeh0XO0INUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
      ],
      "metadata": {
        "id": "4WH_dERBINcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=256, validation_split=0.2)"
      ],
      "metadata": {
        "id": "6iE9T4vcISO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "metadata": {
        "id": "02ND0kU9d-FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('best_model.h5')"
      ],
      "metadata": {
        "id": "fy7oJoeYX_hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('best_model.h5')"
      ],
      "metadata": {
        "id": "Btw4fhMIYH_B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}