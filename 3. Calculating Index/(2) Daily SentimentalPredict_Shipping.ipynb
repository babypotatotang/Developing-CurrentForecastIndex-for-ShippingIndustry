{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kthink03/KMI-Project_tmp/blob/master/Source/Analysis%20Sentimental/SentimentalPredict_Shipping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7ybiLwZaAPZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gArAvtKGgY31"
      },
      "outputs": [],
      "source": [
        "News_Data=pd.read_excel(filename)\n",
        "News_Data.columns=['Index','Date','Content'] # 칼럼 명 변경 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcKd-Q45gB-j"
      },
      "outputs": [],
      "source": [
        "Count=pd.read_excel(filename)\n",
        "LCount=Count['뉴스 개수'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmvr3bcEgYfM"
      },
      "outputs": [],
      "source": [
        "Content=News_Data['Content'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP2WOatjhCFy"
      },
      "outputs": [],
      "source": [
        "TotalContent=[] #월 별로 키워드를 append하여 저장 \n",
        "for i in LCount:\n",
        "    if len(TotalContent)==0:\n",
        "        TotalContent.append(Content[0:i])\n",
        "        pre=i\n",
        "    else:\n",
        "        TotalContent.append(Content[pre:pre+i])\n",
        "        pre+=i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMyk-U-8gpPX"
      },
      "outputs": [],
      "source": [
        "loaded_model = load_model(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sc3IqHFgyfK2"
      },
      "outputs": [],
      "source": [
        "X_train=np.load(filename,allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzfnfd0Yhlf1"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "threshold = 6\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
        "\n",
        "# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\n",
        "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
        "vocab_size = total_cnt - rare_cnt + 2\n",
        "print('단어 집합의 크기 :',vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY-hmRmAsBML"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
        "tokenizer.fit_on_texts(X_train)\n",
        "max_len=1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GFteMVxhAZ5"
      },
      "outputs": [],
      "source": [
        "def sentiment_predict(new_sentence):\n",
        "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
        "\n",
        "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
        "    score = float(loaded_model.predict(pad_new)) # 예측\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caHuXQ5Ah98-"
      },
      "outputs": [],
      "source": [
        "List_Pos=[]; List_Neg=[]\n",
        "\n",
        "for i in tqdm(TotalContent):\n",
        "    score=sentiment_predict(i)\n",
        "    List_Pos.append(score); List_Neg.append(1-score)\n",
        "\n",
        "data={'Pos':List_Pos,'Neg':List_Neg}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6JwCK5LOWhK"
      },
      "outputs": [],
      "source": [
        "df_score=pd.DataFrame(data,columns=['Pos','Neg'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s45NrHEZnuoK"
      },
      "outputs": [],
      "source": [
        "senti=[]\n",
        "for index,row in df_score.iterrows():\n",
        "    senti.append(row['Pos']-row['Neg'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ58MjX1oB1o"
      },
      "outputs": [],
      "source": [
        "senti=pd.DataFrame(senti,columns=['Senti'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-y5CR0YOghd"
      },
      "outputs": [],
      "source": [
        "df_total=pd.concat([df_score,senti],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiRYuEJ0PjDU"
      },
      "outputs": [],
      "source": [
        "df_total.to_excel(filename)\n",
        "\n",
        "files.download(filename)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM4OQCy0+z2EFeOv6I3NcaG",
      "include_colab_link": true,
      "name": "Untitled1.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
